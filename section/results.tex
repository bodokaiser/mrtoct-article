\section{Experiments}

We had difficulties with the implementation of the patch aggregation required
for the implementation of the context-aware 3D synthesis. Though patch
aggregation worked in general, it occupied more computational resources than
the actual training. Unfortunatly we were not able to find a workaround or to
allocate more computation quota. Therefor we did not perform more than one
iteration of the auto-context model, which prevented a fair comparison.
However, we encourage everyone to test our implementation themselves.
As a consequence we only conducted experiments with the u-net generator and
the pixtopix \gls{gan}.

\subsection{Distance Losses}

One experimental parameter of interest was the performance impact of the
different loss functions. We first optimized the loss function used when
only training the u-net. Let $X,Y\in{[0,1]}^N$ be output and target vectors,
then we define the \gls{mae} to be
\begin{equation}
  \mae\left(X,Y\right)
  =
  \frac{1}{N}\sum_{i=1}^N
  \abs{X_i-Y_i}
  \label{eq:mae}.
\end{equation}
The \gls{mse} we define via
\begin{equation}
  \mse\left(X,Y\right)
  =
  \frac{1}{N}\sum_{i=1}^N
  {\left(X_i-Y_j\right)}^2
  \label{eq:mse}.
\end{equation}
Finally the \gls{gdl} disclosed in Ref.~\cite{Nie16} is defined as
\begin{equation}
  \gdl\left(X,Y\right)
  =
  \mse\left(\grad X,\grad Y\right)
  \label{eq:gdl},
\end{equation}
wherein $\grad$ is the spatial gradient. We approximate the $i$th element
of the spatial gradient through
\begin{equation}
  \grad X_i
  \approx
  \begin{cases}
    X_i-X_{i+1}\qc\text{if }, $1<i<N$\\
    0\qc\text{otherwise}
  \end{cases}
  \label{eq:grad}.
\end{equation}
The loss terms can of course be combined
\begin{equation}
  \lambda_\text{MAE}\mae\left(X,Y\right)
  +
  \lambda_\text{MSE}\mse\left(X,Y\right)
  +
  \lambda_\text{GDL}\gdl\left(X,Y\right),
\end{equation}
wherein the $\lambda$ denotes the weight of the respective loss term.

\begin{table}[h]
  \centering
  \begin{tabular}{ccc}
    \toprule
    Dataset & Subject & Shape \\
    \midrule
    Training & \num{1} & \num{161x320x250x1} \\
    Training & \num{2} & \num{149x328x250x1} \\
    Training & \num{3} & \num{112x303x281x1} \\
    Training & \num{4} & \num{155x291x259x1} \\
    Training & \num{5} & \num{143x307x284x1} \\
    Training & \num{6} & \num{149x278x267x1} \\
    Training & \num{7} & \num{200x289x268x1} \\
    Training & \num{8} & \num{218x282x238x1} \\
    Training & \num{9} & \num{191x322x252x1} \\
    Training & \num{10} & \num{200x303x243x1} \\
    Training & \num{11} & \num{181x317x239x1} \\
    Training & \num{12} & \num{186x310x248x1} \\
    Training & \num{13} & \num{112x313x238x1} \\
    Validation & \num{1} & \num{112x298x227x1} \\
    Validation & \num{2} & \num{223x328x282x1} \\
    Validation & \num{3} & \num{223x307x276x1} \\
    Validation & \num{4} & \num{204x329x262x1} \\
    \bottomrule
  \end{tabular}
  \caption{Training and validation dataset volumes used in this section. The
    dimensions of the shape correspond to depth, height and width.
  }\label{tab:unet:dataset}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    & &
    \multicolumn{2}{c}{\acrshort{mae}} &
    \multicolumn{2}{c}{\acrshort{mae}+\acrshort{gdl}} \\
    Image Slices & Batch Size & Steps & Epochs & Steps & Epochs \\
    \midrule
    \num{2157} & \num{16} & \num{20542} & \num{152} & \num{19388} & \num{143} \\
    \bottomrule
  \end{tabular}
  \caption{Training parameters used for the distance metrics experiments.
  }\label{tab:unet:params}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    $\lambda_\text{mae}$ &
    $\lambda_\text{gdl}$ &
    \acrshort{mae} &
    \acrshort{mse} &
    \acrshort{psnr} \\
    \midrule
    \num{1} & \num{0} & \num{31.58} & \num{6577} & \num{59.5} \\
    \num{1} & \num{1e-7} & \num{37.15} & \num{7945} & \num{58.1} \\
    \bottomrule
  \end{tabular}
  \caption{Distance metrics for the u-net model trained with different loss
    functions, evaluated on the training dataset.
  }\label{tab:unet:training}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    $\lambda_\text{mae}$ &
    $\lambda_\text{gdl}$ &
    \acrshort{mae} &
    \acrshort{mse} &
    \acrshort{psnr} \\
    \midrule
    \num{1} & \num{0} & \num{123.6} & \num{70846} & \num{47.93} \\
    \num{1} & \num{1e-7} & \num{129.0} & \num{72704} & \num{47.90} \\
    \bottomrule
  \end{tabular}
  \caption{Distance metrics for the u-net model trained with different loss
    functions, evaluated on the validation dataset.
  }\label{tab:unet:validation}
\end{table}
\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
    \inputpgf{figure}{unet-training.pgf}
  \end{adjustbox}
  \caption{Transverse views for the u-net model trained with different
    loss functions, evaluated on the training dataset.
  }\label{fig:unet:training}
\end{figure}
\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
    \inputpgf{figure}{unet-validation.pgf}
  \end{adjustbox}
  \caption{Transverse views for the u-net model trained with different
    loss functions, evaluated on the validation dataset.
  }\label{fig:unet:validation}
\end{figure}

\subsection{Adversarial Losses}

\begin{table}[h]
  \centering
  \begin{tabular}{ccc}
    \toprule
    Dataset & Subject & Shape \\
    \midrule
    Training & \num{1} & \num{137x320x250x1} \\
    Training & \num{2} & \num{130x328x250x1} \\
    Training & \num{3} & \num{111x303x281x1} \\
    Training & \num{4} & \num{143x291x259x1} \\
    Training & \num{5} & \num{141x307x284x1} \\
    Training & \num{6} & \num{148x278x267x1} \\
    Training & \num{7} & \num{198x289x268x1} \\
    Training & \num{8} & \num{208x282x238x1} \\
    Training & \num{9} & \num{162x322x252x1} \\
    Training & \num{10} & \num{185x303x243x1} \\
    Training & \num{11} & \num{180x317x239x1} \\
    Training & \num{12} & \num{184x310x248x1} \\
    Training & \num{13} & \num{93x313x238x1} \\
    Validation & \num{1} & \num{105x298x227x1} \\
    Validation & \num{2} & \num{190x328x282x1} \\
    Validation & \num{3} & \num{202x307x276x1} \\
    Validation & \num{4} & \num{198x329x262x1} \\
    \bottomrule
  \end{tabular}
  \caption{Training and validation dataset volumes used in this section. The
    dimensions of the shape correspond to depth, height and width.
  }\label{tab:unet_pixtopix:dataset}
\end{table}

\begin{table}[h]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    & &
    \multicolumn{2}{c}{u-net (\gls{mae})} &
    \multicolumn{2}{c}{pixtopix} \\
    Image Slices & Batch Size & Steps & Epochs & Steps & Epochs \\
    \midrule
    \num{2020} & \num{16} & \num{22080} & \num{174} & \num{37832} & \num{299} \\
    \bottomrule
  \end{tabular}
  \caption{Training parameters used for the u-net and pixtopix comparison.
  }\label{tab:unet_pixtopix:params}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Model & Loss &
    \acrshort{mae} &
    \acrshort{mse} &
    \acrshort{psnr} \\
    \midrule
    u-net & \acrshort{mae} & \num{90.5} & \num{61853} & \num{49.4} \\
    pixtopix & least-square & \num{21.6} & \num{4210} & \num{60.4} \\
    \bottomrule
  \end{tabular}
  \caption{Distance metrics for the u-net model trained with \acrshort{mae}
    loss compared with the pixtopix model trained with least-square adversarial
    loss, evaluated on the training dataset.
  }\label{tab:unet_pixtopix:training}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Model & Loss &
    \acrshort{mae} &
    \acrshort{mse} &
    \acrshort{psnr} \\
    \midrule
    u-net & \acrshort{mae} & \num{136.9} & \num{101943} & \num{46.77} \\
    pixtopix & least-square & \num{112.7} & \num{82173} & \num{47.55} \\
    \bottomrule
  \end{tabular}
  \caption{Distance metrics for the u-net model trained with \acrshort{mae}
    loss compared with the pixtopix model trained with least-square adversarial
    loss, evaluated on the validation dataset.
  }\label{tab:unet_pixtopix:training}
\end{table}
\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
    \inputpgf{figure}{unet-pixtopix-training.pgf}
  \end{adjustbox}
  \caption{Transverse views for the u-net model trained with \acrshort{mae}
    loss compared with the pixtopix model trained with least-square adversarial
    loss, evaluated on the training dataset.
  }\label{fig:unet_pixtopix:training}
\end{figure}
\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
    \inputpgf{figure}{unet-pixtopix-validation.pgf}
  \end{adjustbox}
  \caption{Transverse views for the u-net model trained with \acrshort{mae}
    loss compared with the pixtopix model trained with least-square adversarial
    loss, evaluated on the validation dataset.
  }\label{fig:unet_pixtopix:validation}
\end{figure}

\subsection{Gradient Boost}

\begin{table}[h]
  \centering
  \begin{tabular}{cccccc}
    \toprule
    & &
    \multicolumn{2}{c}{u-net (\gls{mae})} &
    \multicolumn{2}{c}{pixtopix} \\
    Image Slices & Batch Size & Steps & Epochs & Steps & Epochs \\
    \midrule
    \num{2020} & \num{16} & \num{37832} & \num{299} & \num{51911} & \num{411} \\
    \bottomrule
  \end{tabular}
  \caption{Training parameters used for pixtopix and gradient boosted
    fine-tuned pixtopix model.
  }\label{tab:pixtopix:params}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Model & Loss &
    \acrshort{mae} &
    \acrshort{mse} &
    \acrshort{psnr} \\
    \midrule
    pixtopix & least-square & \num{21.56} & \num{4210} & \num{60.38} \\
    pixtopix (boosted) & least-square & \num{23.37} & \num{4140} & \num{60.30} \\
    \bottomrule
  \end{tabular}
  \caption{Distance metrics for the pixtopix model trained with least-square
    adversarial loss compared to fine-tuned with gradient boost, evaluated on
    the training dataset.
  }\label{tab:pixtopix:training}
\end{table}
\begin{table}[h]
  \centering
  \begin{tabular}{ccccc}
    \toprule
    Model & Loss &
    \acrshort{mae} &
    \acrshort{mse} &
    \acrshort{psnr} \\
    \midrule
    pixtopix & least-square & \num{112.73} & \num{82173} & \num{47.55} \\
    pixtopix & least-square & \num{112.23} & \num{79296} & \num{47.68} \\
    \bottomrule
  \end{tabular}
  \caption{Distance metrics for the pixtopix model trained with least-square
    adversarial loss compared to fine-tuned with gradient boost, evaluated on
    the validation dataset.
  }\label{tab:pixtopix:validation}
\end{table}
\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
    \inputpgf{figure}{pixtopix-training.pgf}
  \end{adjustbox}
  \caption{Transverse views for the pixtopix model trained with least-square
    adversarial loss compared to fine-tuned with gradient boost, evaluated on
    the validation dataset.
  }\label{fig:pixtopix:training}
\end{figure}
\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
    \inputpgf{figure}{pixtopix-validation.pgf}
  \end{adjustbox}
  \caption{Transverse views for the pixtopix model trained with least-square
    adversarial loss compared to fine-tuned with gradient boost, evaluated on
    the validation dataset.
  }\label{fig:pixtopix:validation}
\end{figure}

